<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>URL Crawler | Lucien Baumgartner</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="URL Crawler" />
<meta name="author" content="Lucien Baumgartner" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The following procedure has been developed to provide a stable URL collection method than can be coupled with text extraction and text burning features. The method was used for this research proposal for SocialScienceOne. The main goal of the workflow is to generate a corpus of URLs related to online media content for all Swiss referanda from June, 2017 to March, 2018, including their text body. The starting point for the URL collection is a list of the top Google searches for each initiative prior to the respective ballots. From there, the algorithm accesses all URLs and extracts all potential links to other informational content on the referendum, this is hyperreferences indluded on those pages. To avoid including links to web-ads or other unrelated content we applied a keyword filter to the newly scraped URLs. The new URLs are then matched to the ones from the initial collection. Only the new entries are then accessed in turn, to extract all possible references, and so on, until there is no new reference or a time limit has been reached." />
<meta property="og:description" content="The following procedure has been developed to provide a stable URL collection method than can be coupled with text extraction and text burning features. The method was used for this research proposal for SocialScienceOne. The main goal of the workflow is to generate a corpus of URLs related to online media content for all Swiss referanda from June, 2017 to March, 2018, including their text body. The starting point for the URL collection is a list of the top Google searches for each initiative prior to the respective ballots. From there, the algorithm accesses all URLs and extracts all potential links to other informational content on the referendum, this is hyperreferences indluded on those pages. To avoid including links to web-ads or other unrelated content we applied a keyword filter to the newly scraped URLs. The new URLs are then matched to the ones from the initial collection. Only the new entries are then accessed in turn, to extract all possible references, and so on, until there is no new reference or a time limit has been reached." />
<link rel="canonical" href="http://localhost:4000/projects/2018/07/03/url-crawler.html" />
<meta property="og:url" content="http://localhost:4000/projects/2018/07/03/url-crawler.html" />
<meta property="og:site_name" content="Lucien Baumgartner" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-07-03T10:38:51+02:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/projects/2018/07/03/url-crawler.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/2018/07/03/url-crawler.html"},"author":{"@type":"Person","name":"Lucien Baumgartner"},"headline":"URL Crawler","dateModified":"2018-07-03T10:38:51+02:00","description":"The following procedure has been developed to provide a stable URL collection method than can be coupled with text extraction and text burning features. The method was used for this research proposal for SocialScienceOne. The main goal of the workflow is to generate a corpus of URLs related to online media content for all Swiss referanda from June, 2017 to March, 2018, including their text body. The starting point for the URL collection is a list of the top Google searches for each initiative prior to the respective ballots. From there, the algorithm accesses all URLs and extracts all potential links to other informational content on the referendum, this is hyperreferences indluded on those pages. To avoid including links to web-ads or other unrelated content we applied a keyword filter to the newly scraped URLs. The new URLs are then matched to the ones from the initial collection. Only the new entries are then accessed in turn, to extract all possible references, and so on, until there is no new reference or a time limit has been reached.","datePublished":"2018-07-03T10:38:51+02:00","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Lucien Baumgartner" /></head>
<body>

<!--<header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Lucien Baumgartner</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/category/projects.html">Projects</a></div>
      </nav></div>
</header>
-->
    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">URL Crawler</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-07-03T10:38:51+02:00" itemprop="datePublished">Jul 3, 2018
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Lucien Baumgartner</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">URL Crawler</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-07-03T10:38:51+02:00" itemprop="datePublished">Jul 3, 2018
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Lucien Baumgartner</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>The following procedure has been developed to provide a stable URL collection method than can be coupled with text extraction and text burning features. The method was used for this research proposal for SocialScienceOne. The main goal of the workflow is to generate a corpus of URLs related to online media content for all Swiss referanda from June, 2017 to March, 2018, including their text body. The starting point for the URL collection is a list of the top Google searches for each initiative prior to the respective ballots. From there, the algorithm accesses all URLs and extracts all potential links to other informational content on the referendum, this is hyperreferences indluded on those pages. To avoid including links to web-ads or other unrelated content we applied a keyword filter to the newly scraped URLs. The new URLs are then matched to the ones from the initial collection. Only the new entries are then accessed in turn, to extract all possible references, and so on, until there is no new reference or a time limit has been reached.</p>

  </div><a class="u-url" href="/projects/2018/07/03/url-crawler.html" hidden></a>
</article>
-->


<!DOCTYPE html>
<html lang="en">

<body>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">
          <div class="content">
            <p>The following procedure has been developed to provide a stable URL collection method than can be coupled with text extraction and text burning features. The method was used for this research proposal for SocialScienceOne. The main goal of the workflow is to generate a corpus of URLs related to online media content for all Swiss referanda from June, 2017 to March, 2018, including their text body. The starting point for the URL collection is a list of the top Google searches for each initiative prior to the respective ballots. From there, the algorithm accesses all URLs and extracts all potential links to other informational content on the referendum, this is hyperreferences indluded on those pages. To avoid including links to web-ads or other unrelated content we applied a keyword filter to the newly scraped URLs. The new URLs are then matched to the ones from the initial collection. Only the new entries are then accessed in turn, to extract all possible references, and so on, until there is no new reference or a time limit has been reached.</p>

        </div>
      </div>
</main>

</body>

</html>

  </div><a class="u-url" href="/projects/2018/07/03/url-crawler.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Contact me!</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Lucien Baumgartner</li><li><a class="u-email" href="mailto:luciengeorges.baumgartner@uzh.ch">luciengeorges.baumgartner@uzh.ch</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/lucienbaumgartner"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">lucienbaumgartner</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

    </div>

  </div>

</footer>
</body>

</html>
